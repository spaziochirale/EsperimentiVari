{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNKRPV3od6T/gmoV+wjuloi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/spaziochirale/EsperimentiVari/blob/main/PrototipoRAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prototipo di ChatBot RAG con LangChain\n",
        "\n",
        "Questo notebook mostra come realizzare un *chatBot* di tipo **RAG** utilizzando il framework **LangChain**."
      ],
      "metadata": {
        "id": "xSb_t-bqnMrf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Per prima cosa installiamo sul server alcuni package Python della piattaforma LangChain che utilizzeremo nel prototipo.\n",
        "\n",
        "***langchain*** è il modulo principale, ***langchain_community*** è il modulo che contiene i contributi di terze parti e della community al progetto LangChain e ***langchain_chroma*** è il modulo che contiene l'interfaccia verso il dtatabase vettoriale open source **Chroma**, usato in questo esempio.\n",
        "\n",
        "Il punto esclamativo prima del comando serve a fare in modo che questo comando sia una istruzione eseguita dal Sistema Operativo Linux del server virtuale Google Colab. Normalmente le celle di un notebook come questo contengono istruzioni in linguaggio Python che vengono eseguite dall'interprete Python.\n",
        "\n",
        "Pip è il **gestore dei pacchetti** di Python e serve per scaricare dai repository ufficiali le librerie e installarle sul server locale. Pip si esegue da linea di comando direttamente sul terminale del server. Il server virtuale creato da Google Colab non contiene le librerie di LangChain, nè altre librerie specifiche di terze parti, per cui vanno installate tramite l'utility **pip**."
      ],
      "metadata": {
        "id": "81CAjqFnnh_b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X12-HGndcj5v"
      },
      "outputs": [],
      "source": [
        "!pip install langchain langchain_community langchain_chroma"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se vogliamo utilizzare i LLM di OpenAI, dobbiamo installare il modulo specifico di LangChain che interfaccia le API di OpenAI.\n",
        "L'istruzione pip che segue effettua questa installazione. le opzioni -qU eliminano l'output a video e forzano l'upgrade alla versione più aggiornata qualora il modulo fosse già aggiornato."
      ],
      "metadata": {
        "id": "YDQGv2NWpfAP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-openai"
      ],
      "metadata": {
        "id": "9odXcskgdjLe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nella cella di codice che segue viene impostata sul server la variabile di ambiente OPENAI_API_KEY, il cui valore viene prelevato dalla sezione \"**secrets**\" del Colab. Per poter utilizzare questo notebook, occorre quindi creare un secret colab di etichetta OPENAI_API_KEY e valorizzarla con una chiave OpenAI valida.\n",
        "\n",
        "Successivamente viene creato un oggetto chiamato **llm** che rappresenta l'interfaccia LangChain verso il modello **gpt-4o-mini** di OpenaAI."
      ],
      "metadata": {
        "id": "Y11z0LG1qfp7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
      ],
      "metadata": {
        "id": "HaQjPajReTIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La cella successiva contiene le istruzioni import per importare nel nostro programma tutte le librerie Python che saranno utilizzate.\n",
        "\n",
        "- bs4 è una nota libreria open source, chiamata Beautiful Soup, che facilita le operazioni di web scraping e parsing di pagine HTML dai siti web.\n",
        "- hub è un modulo di LangChain che contiene risorse ed esempi già pronti che velocizzano lo sviluppo. Nel nostro programma servirà per recuperare un template di prompt per il LLM, ben strutturato per il RAG.\n",
        "- Chroma è un database vettoriale open source molto leggero e flessibile.\n",
        "- WebBaseLoaderè un modulo LangChain, di tipo document loader, sviluppato dalla community per importare pagine dal web.\n",
        "- StrOutParser è un modulo di langChain per semplificare le operazioni di parsing su stringhe complesse\n",
        "- RunnablePassthrough è un modulo LangChain che automatizza il passaggio di valori e parametri tra i passi contigui in una chain. Si veda la documentazione LangChain per dettagli su questi concetti.\n",
        "- OpenAIEmbeddings è il modulo LangChain di interfaccia alle API Embeddings di OpenAI\n",
        "- RecursiveCharacterTextSplitter è un modulo LangChain per effettuare l'operazione di spezzettamento in chunck di stringhe di testo corrispondenti a testi molto lunghi."
      ],
      "metadata": {
        "id": "oADDwAC3r7oH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import bs4\n",
        "from langchain import hub\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n"
      ],
      "metadata": {
        "id": "QdDVr0Acfk5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il frammento di codice che segue effettua lo scraping di una pagina web.\n",
        "La pagina catturata da Internet viene trasformata in una grande e unica stringa di testo posta nella variabile docs.\n",
        "Un loader LangChain normalmente crea una lista di documenti di tipo testo. La variabile docs è quindi una lista Python ma nel nostro caso contiene un solo elemento."
      ],
      "metadata": {
        "id": "5nRULG7MvBlk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://www.chirale.it/fotocamere-antiche-come-costruire-con-arduino-un-misuratore-di-velocita-dellotturatore/\",),\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "docs = loader.load()\n",
        "\n"
      ],
      "metadata": {
        "id": "kyI-hIfIuxbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Proviamo ad esplorare quello che è successo.\n",
        "Vediamo qual'è il tipo della variabile docs e stampiamo il suo contenuto."
      ],
      "metadata": {
        "id": "_C-4pm6R3zOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# L'esecuzione di questa cella è opzionale\n",
        "print('Tipo della variabile:',type(docs))\n",
        "print(docs)"
      ],
      "metadata": {
        "id": "0m6BgN-X3ODV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il codice della cella che segue crea un oggetto, chiamato **text_splitter** di tipo ***RecursiveCharacterTextSplitter*** con dimensioni dei chunk pari a 1.000 caratteri e una sovrapposizione tra chunk di 200 caratteri.\n",
        "\n",
        "La stringa contenuta nella variabile **`docs`** viene spezzata in chunks usando lo splitter definito e il risultato, cioè la lista dei singoli frammenti di testo (chunk) è memorizzata nella variabile **splits**.\n",
        "\n",
        "Infine, viene creato l'oggetto **vectorstore** come database Chroma contenente i chunk vettorializzati tramite l'embedding di OpenAI e indicizzati.\n",
        "Il metodo **Chroma.from_documents** di LangChain fa esattamente questo con una sola chiamata."
      ],
      "metadata": {
        "id": "wPHXvQtNvzDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "splits = text_splitter.split_documents(docs)\n",
        "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
        "\n"
      ],
      "metadata": {
        "id": "zDGR1ZCJvpTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Anche in questo caso proviamo a vedere il contenuto della variabile splits ed esploriamo il suo tipo e la sua struttura."
      ],
      "metadata": {
        "id": "tcpOu-ep4vsW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# L'esecuzione di questa cella di codice è opzionale\n",
        "print('Tipo della variabile:',type(splits))\n",
        "print('lunghezza di splits:', len(splits))\n",
        "print(splits[0])\n",
        "print(splits[1])"
      ],
      "metadata": {
        "id": "ccj2iaBd4-Ra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nella cella seguente, viene creato l'oggetto **retriever** invocando il metodo **as_retriever()** dell'oggetto **vectorstore**.\n",
        "Questa è un'altra delle caratteristiche che mostrano la potenza di LangChain. In LangChain gli oggetti che rappresentano l'interfaccia verso un database vettoriale possono essere utilizzati anche come ***retriever***.\n",
        "I *retriever* LangChain sono degli oggetti specializzati nel recupero di testo mediante una ricerca semantica sul database vettoriale.\n",
        "Semplificano il passaggio di una stringa contenente la *query*, la sua vettorializzazione mediante *embeddings* e il recupero dei chunk di testo più pertinenti dal database vettoriale.\n",
        "\n",
        "Successivamente viene creato l'oggetto **prompt** contenente il ***prompt template*** adatto al nostro caso, recuperato dall'hub di LangChain.\n",
        "\n",
        "I *prompt template* sono un'altra delle potenzialità offerta da LangChain. Si veda la documentazione."
      ],
      "metadata": {
        "id": "seUQmPwQx0eF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vectorstore.as_retriever()\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n"
      ],
      "metadata": {
        "id": "u9cefv8qxl5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nel seguito viene definita una semplice funzione di utilità per formattare come stringa multiriga una sequenza di testi."
      ],
      "metadata": {
        "id": "qmFGhwqazquP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n"
      ],
      "metadata": {
        "id": "SJh4SC4rzmJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ed ecco, la classica definizione della chain di LangChain, dove attraverso il suo formalismo *LCEL*, viene definita la sequenza di operazioni da effettuare.\n",
        "Il primo elemento della chain è un dizionario Python in cui alla chiave \"**context**\" viene associato il risultato di una catena di due operazioni: il valore dell'oggetto **retriever** formattato dalla funzione di utilità che abbiamo definito poco sopra.\n",
        "Alla chiave \"**question**\" viene associato il valore restituito dal metodo **RunnablePassThgrough**, cioè il valore inserito quando la chain sarà invocata.\n",
        "Questo dizionario sarà passato al *prompt template*, **prompt**, che quindi definirà il prompt effettivo, formattato in modo corretto.\n",
        "Tale prompt viene quindi passato all'oggetto **llm**, che invocherà le API di Chat di OpenAI e l'output risultante sarà passato al metodo ***StrOutputParser*** che estrarrà il messaggio di risposta.\n"
      ],
      "metadata": {
        "id": "hbBkNx9wz_UJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n"
      ],
      "metadata": {
        "id": "YKFylSs_z57q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A questo punto possiamo invocare la chain rag_chain passando la query. Il sistema risponderà secondo il contenuto del nostro sito web."
      ],
      "metadata": {
        "id": "74J1v0x_2MWU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "rag_chain.invoke(\"Dove posso acquistare oggigiorno lastre fotografiche in vetro?\")"
      ],
      "metadata": {
        "id": "_gs_1L2rhuId"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}